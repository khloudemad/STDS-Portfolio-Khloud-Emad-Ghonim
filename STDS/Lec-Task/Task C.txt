1. Upload to Raw Storage

The record arrives as part of a raw CSV file (e.g., meter_id=123, timestamp=2025-12-17T12:00:00, energy=500, unit=W) from the smart meter via API or batch upload. It's stored unchanged in a raw storage bucket (object storage like AWS S3). This serves as the "landing zone" for dark data, preserving the original format for auditing and compliance.



2. Triggering of the Transformation Process

The file upload event automatically triggers serverless orchestration (e.g., AWS Lambda). The orchestrator parses the CSV, extracts individual records, and invokes the transformation layer. If the initial trigger fails (due to high load or network issues), it automatically retries up to 3 times before logging an error for manual intervention.



3. Data Cleaning and Validation Steps

In the transformation function:

Parsing: Extract fields from the CSV record

Unit Standardization: Convert 500 W â†’ 0.5 kW (divide by 1000)

Null Checking: Verify no missing values

Range Validation: Ensure energy value is within acceptable limits (0-50 kW for residential)

Fault Detection: Check time-series context; if part of 24+ hour zero-streak, flag as potentially faulty

Metadata Enrichment: Add cleaned timestamp (UTC), processing ID, and quality score
If validation fails (e.g., invalid range or format), the record is flagged and routed to the error path.



4. Storage in Structured Format (RDS)

The cleaned record is loaded into a structured database (RDS table like cleaned_readings with columns: meter_id, timestamp, energy_kW, flags, quality_score). This enables immediate SQL queries for validation (peak detection) and real-time insights. If the insert fails (database connection issue), the system retries 3 times with exponential backoff; on persistent failure, sends to Dead Letter Queue (DLQ) and triggers an alert.




5. Conversion and Archival in Parquet Format

Simultaneously, the record is converted to Parquet format (optimized for columnar analytics) and appended to partitioned files in archival storage (organized by date/hour/meter_id). Parquet automatically handles compression (reducing storage by ~80%) and schema enforcement, preparing the data for long-term analysis like forecasting and trend detection.




6. How Success or Failure is Handled

On Success: The orchestrator logs completion metrics. The record becomes available in both RDS (for operational queries) and Parquet storage (for analytical queries), enabling peak detection and dashboard updates immediately.

On Failure (during transform/validation): The system retries the failed step up to 3 times with exponential backoff. If all retries fail, the record moves to a DLQ for manual inspection, triggers an alert (email/SNS), and is excluded from downstream analytics to prevent bad data propagation. This ensures pipeline resilience with minimal manual intervention.

